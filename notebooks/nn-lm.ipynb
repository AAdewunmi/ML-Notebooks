{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Models\n",
    "Status of Notebook: Work in Progress\n",
    "\n",
    "Reference: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dynet Version: https://github.com/neubig/nn4nlp-code/blob/master/02-lm/nn-lm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download the datasets\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/test.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/train.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in data, process each line and split columns by \" ||| \"\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\" \")\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "# read the data\n",
    "train_data = read_data('data/ptb/train.txt')\n",
    "val_data = read_data('data/ptb/valid.txt')\n",
    "\n",
    "# creating the word and tag indices and special tokens\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_to_index[\"<s>\"] = len(word_to_index)\n",
    "index_to_word[len(word_to_index)-1] = \"<s>\"\n",
    "word_to_index[\"<unk>\"] = len(word_to_index) # add <UNK> to dictionary\n",
    "index_to_word[len(word_to_index)-1] = \"<unk>\"\n",
    "\n",
    "# create word to index dictionary and tag to index dictionary from data\n",
    "def create_dict(data, check_unk=False):\n",
    "    for line in data:\n",
    "        for word in line:\n",
    "            if check_unk == False:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = len(word_to_index)\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "            \n",
    "            # has no effect because data already comes with <unk>\n",
    "            # should work with data without <unk> already processed\n",
    "            else: \n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = word_to_index[\"<unk>\"]\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "\n",
    "create_dict(train_data)\n",
    "create_dict(val_data, check_unk=True)\n",
    "\n",
    "# create word and tag tensors from data\n",
    "def create_tensor(data):\n",
    "    for line in data:\n",
    "        yield([word_to_index[word] for word in line])\n",
    "\n",
    "train_data = list(create_tensor(train_data))\n",
    "val_data = list(create_tensor(val_data))\n",
    "\n",
    "number_of_words = len(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation we are using batched training. There are a few differences from the original implementation found [here](https://github.com/neubig/nn4nlp-code/blob/master/02-lm/loglin-lm.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "N = 2 # length of the n-gram\n",
    "EMB_SIZE = 128 # size of the embedding\n",
    "HID_SIZE = 128 # size of the hidden layer\n",
    "\n",
    "# Neural LM\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, number_of_words, ngram_length, EMB_SIZE, HID_SIZE):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(number_of_words, EMB_SIZE)\n",
    "\n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(EMB_SIZE * ngram_length, HID_SIZE)\n",
    "        # output layer\n",
    "        self.output = nn.Linear(HID_SIZE, number_of_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.embedding(x) # [batch_size x num_hist x emb_size]\n",
    "        embs = embs.view(embs.size(0), -1) # [batch_size x (num_hist*emb_size)]\n",
    "        h = torch.nn.functional.tanh(self.hidden(embs)) # batch_size x hid_size\n",
    "        scores = self.output(h) # batch_size x num_words\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(number_of_words, N, EMB_SIZE, HID_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "\n",
    "# function to calculate the sentence loss\n",
    "def calc_sent_loss(sent):\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    \n",
    "    # initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    \n",
    "    # collect all target and histories\n",
    "    all_targets = []\n",
    "    all_histories = []\n",
    "    \n",
    "    # step through the sentence, including the end of sentence token\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "\n",
    "    logits = model(torch.LongTensor(all_histories).to(device))\n",
    "    loss = criterion(logits, torch.LongTensor(all_targets).to(device))\n",
    "\n",
    "    return torch.sum(loss)\n",
    "\n",
    "MAX_LEN = 100\n",
    "# Function to generate a sentence\n",
    "def generate_sent():\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = model(torch.LongTensor([hist]).to(device))\n",
    "        p = torch.nn.functional.softmax(logits) # 1 x number_of_words\n",
    "        next_word = p.multinomial(num_samples=1).item()\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 0: train loss/word=4.3265, ppl=75.6790\n",
      "iter 0: dev loss/word=4.4869, ppl=88.8440, time=1.42s\n",
      "the next healthvest the club of lying <unk> student <unk> could <unk>\n",
      "the next healthvest the club of lying <unk> student <unk> could <unk> and a company at a\n",
      "the next healthvest the club of lying <unk> student <unk> could <unk> <unk> <unk> as <unk> williams <unk> student <unk> could <unk> and <unk> around <unk> ruth <unk> student <unk> student <unk> could <unk> and <unk> with <unk> student <unk> could <unk> and <unk> company <unk> on <unk> but <unk> each <unk> student <unk> could <unk> <unk> as <unk> williams <unk> student <unk> could <unk> and a company at a a <unk> student <unk> could <unk> <unk> as <unk> williams <unk> student <unk> could <unk> and <unk> around <unk> ruth <unk> student <unk> could <unk> and <unk> around <unk> ruth\n",
      "state to there investors stop takes visited for restructuring <unk> student <unk> could <unk> and a company at a\n",
      "the next healthvest the club of lying <unk> student <unk> could <unk> <unk> as <unk> williams <unk> student <unk> could <unk> and <unk> around <unk> ruth <unk> student <unk> could <unk> and a company at a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 1: train loss/word=4.4450, ppl=85.2025\n",
      "iter 1: dev loss/word=4.5506, ppl=94.6892, time=1.41s\n",
      "japanese the quarter unit 's adrs <unk> n.c that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new york banks release respondents that it describes new\n",
      "japanese the quarter unit to a blast of his mortgage commerce manages undeveloped was processing N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N\n",
      "japanese the quarter unit 's adrs <unk> n.c that it describes at the company 's coal <unk>\n",
      "japanese the quarter unit office the installation are as expected <unk> formerly nelson are of the evening culture chief hold get county corp. more than\n",
      "japanese the quarter unit to N million N cents included who can too a hard a political change development aid arrested both 's describes the installation at much\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 2: train loss/word=4.4740, ppl=87.7028\n",
      "iter 2: dev loss/word=4.6168, ppl=101.1712, time=1.42s\n",
      "investors for\n",
      "on oct.\n",
      "has oct. some a decent the only the federal of giuliani proposal for mountain-bike in japan\n",
      "in japan\n",
      "has oct. some a decent the only\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 3: train loss/word=4.4896, ppl=89.0867\n",
      "iter 3: dev loss/word=4.5267, ppl=92.4511, time=1.43s\n",
      "bush corp. to at least hospitals more by estimated including computers carl at N million net guilders provide more by estimated including computers carl at large u.s. should center <unk>\n",
      "bush corp. to at least hospitals more by estimated including computers carl at large u.s. should center <unk> an products by N million net guilders provide more by estimated including computers carl at large u.s. should you an abortion who say of relocation <unk> david at metromedia hospitals different the way confirm for ibm 's exposure <unk> exxon a small of the thrift confirm who has copying on grace services chandler germany the way confirm by nonrecurring elements more\n",
      "bush corp. to at least hospitals more and a substantial of the thrift confirm who has copying on grace services chandler germany plants so often the spring confirm of the thrift confirm N million rate\n",
      "bush corp. and a substantial of the thrift confirm research market reported n't nor junk bonds that neither N million rate\n",
      "bush corp. to at least hospitals more by estimated including computers carl at large n't the system confirm plan turkey buys N million rate\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 4: train loss/word=4.5099, ppl=90.9099\n",
      "iter 4: dev loss/word=4.5587, ppl=95.4629, time=1.44s\n",
      "<unk> is state it were bill valuable universal mellon steady rate as members a little visible\n",
      "<unk> unit most the anticipated market economists move and red are currently performed roper escape <unk> away\n",
      "<unk> is state is likely period\n",
      "<unk> unit most the anticipated of\n",
      "<unk> is state it were bill valuable universal mellon steady rate as members a little visible\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 5: train loss/word=4.5167, ppl=91.5292\n",
      "iter 5: dev loss/word=4.7449, ppl=114.9921, time=1.43s\n",
      "seats was for the company 's\n",
      "seats was around population 's previously & fault\n",
      "seats was around all the machinists\n",
      "seats was around population 's previously & fault\n",
      "seats was around to a california initiative provide\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 6: train loss/word=4.5259, ppl=92.3775\n",
      "iter 6: dev loss/word=4.7905, ppl=120.3643, time=1.41s\n",
      "sharing\n",
      "sharing\n",
      "sharing\n",
      "sharing\n",
      "sharing\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 7: train loss/word=4.5397, ppl=93.6627\n",
      "iter 7: dev loss/word=4.7063, ppl=110.6394, time=1.44s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 8: train loss/word=4.5539, ppl=95.0015\n",
      "iter 8: dev loss/word=4.7479, ppl=115.3410, time=1.37s\n",
      "but many <unk> cracks status such never\n",
      "but many <unk> cracks status from\n",
      "but many <unk> cracks status such never and to japan the concern in his statement mr. skase 's international to make a full\n",
      "so of common activity\n",
      "but many <unk> cracks status\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 9: train loss/word=4.5578, ppl=95.3781\n",
      "iter 9: dev loss/word=4.9677, ppl=143.6924, time=1.37s\n",
      "baker <unk> fund led\n",
      "baker <unk> fund led\n",
      "baker <unk> fund led\n",
      "bankers march <unk> jr co. robert\n",
      "bankers march <unk> selected co. presented a pound to go\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for ITER in range (10): # CHANGE to 100\n",
    "    # training\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    for sent_id, sent in enumerate(train_data): # CHANGE to all train_data\n",
    "        \n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        \n",
    "        train_loss += my_loss.item()\n",
    "        train_words += len(sent)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished %r sentences\" % (sent_id+1))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words)))\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(val_data):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.item()\n",
    "        dev_words += len(sent)\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
    "\n",
    "    # Generate a few sentences\n",
    "    for _ in range(5):\n",
    "        sent = generate_sent()\n",
    "        print(\" \".join([index_to_word[x] for x in sent]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
