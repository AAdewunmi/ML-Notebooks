{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Models\n",
    "Status of Notebook: Work in Progress\n",
    "\n",
    "Reference: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "Dynet Version: https://github.com/neubig/nn4nlp-code/blob/master/02-lm/nn-lm.py\n",
    "\n",
    "Additions compared to `nn.lm.ipnyb`:\n",
    "- Cleaned up model architecture code\n",
    "- Added Dropout\n",
    "- Using different initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download the datasets\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/test.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/train.txt\n",
    "#!wget https://raw.githubusercontent.com/neubig/nn4nlp-code/master/data/ptb/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in data, pro=ess each line and split columns by \" ||| \"\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\" \")\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "# read the data\n",
    "train_data = read_data('data/ptb/train.txt')\n",
    "val_data = read_data('data/ptb/valid.txt')\n",
    "\n",
    "# creating the word and tag indices and special tokens\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "word_to_index[\"<s>\"] = len(word_to_index)\n",
    "index_to_word[len(word_to_index)-1] = \"<s>\"\n",
    "word_to_index[\"<unk>\"] = len(word_to_index) # add <UNK> to dictionary\n",
    "index_to_word[len(word_to_index)-1] = \"<unk>\"\n",
    "\n",
    "# create word to index dictionary and tag to index dictionary from data\n",
    "def create_dict(data, check_unk=False):\n",
    "    for line in data:\n",
    "        for word in line:\n",
    "            if check_unk == False:\n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = len(word_to_index)\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "            \n",
    "            # has no effect because data already comes with <unk>\n",
    "            # should work with data without <unk> already processed\n",
    "            else: \n",
    "                if word not in word_to_index:\n",
    "                    word_to_index[word] = word_to_index[\"<unk>\"]\n",
    "                    index_to_word[len(word_to_index)-1] = word\n",
    "\n",
    "create_dict(train_data)\n",
    "create_dict(val_data, check_unk=True)\n",
    "\n",
    "# create word and tag tensors from data\n",
    "def create_tensor(data):\n",
    "    for line in data:\n",
    "        yield([word_to_index[word] for word in line])\n",
    "\n",
    "train_data = list(create_tensor(train_data))\n",
    "val_data = list(create_tensor(val_data))\n",
    "\n",
    "number_of_words = len(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation we are using batched training. There are a few differences from the original implementation found [here](https://github.com/neubig/nn4nlp-code/blob/master/02-lm/loglin-lm.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "N = 2 # length of the n-gram\n",
    "EMB_SIZE = 128 # size of the embedding\n",
    "HID_SIZE = 128 # size of the hidden layer\n",
    "\n",
    "# Neural LM\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, number_of_words, ngram_length, EMB_SIZE, HID_SIZE, dropout):\n",
    "        super(NeuralLM, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(number_of_words, EMB_SIZE)\n",
    "\n",
    "        self.fnn = nn.Sequential(\n",
    "            # hidden layer\n",
    "            nn.Linear(EMB_SIZE * ngram_length, HID_SIZE),\n",
    "            nn.Tanh(),\n",
    "            # dropout layer\n",
    "            nn.Dropout(dropout),\n",
    "            # output layer\n",
    "            nn.Linear(HID_SIZE, number_of_words)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.embedding(x)                        # Size: [batch_size x num_hist x emb_size]\n",
    "        embs = embs.view(embs.size(0), -1)              # Size: [batch_size x (num_hist*emb_size)]\n",
    "        logit = self.fnn(embs)                          # Size: batch_size x num_words                    \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Settings and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(number_of_words, N, EMB_SIZE, HID_SIZE, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)\n",
    "\n",
    "# function to calculate the sentence loss\n",
    "def calc_sent_loss(sent):\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    \n",
    "    # initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    \n",
    "    # collect all target and histories\n",
    "    all_targets = []\n",
    "    all_histories = []\n",
    "    \n",
    "    # step through the sentence, including the end of sentence token\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "\n",
    "    logits = model(torch.LongTensor(all_histories).to(device))\n",
    "    loss = criterion(logits, torch.LongTensor(all_targets).to(device))\n",
    "\n",
    "    return loss\n",
    "\n",
    "MAX_LEN = 100\n",
    "# Function to generate a sentence\n",
    "def generate_sent():\n",
    "    S = word_to_index[\"<s>\"]\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = model(torch.LongTensor([hist]).to(device))\n",
    "        p = torch.nn.functional.softmax(logits) # 1 x number_of_words\n",
    "        next_word = p.multinomial(num_samples=1).item()\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 0: train loss/word=0.2837, ppl=1.3280\n",
      "iter 0: dev loss/word=0.2698, ppl=1.3097, time=1.39s\n",
      "they are principle that\n",
      "since prices rose N for the company is attorney employees fears\n",
      "but what led control\n",
      "N berkeley combining bears the administration 's participation about N N marks off N N after targeted short administration corp. funds today for the nine months that a carrier <unk> national a bell who has n't temporarily that create the market 's request to year-earlier N N utilities at a sale in the first of his aggressive newspapers to close of actions because a central democratic investor regional genetic <unk> volumes\n",
      "then things do n't turning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 1: train loss/word=0.2607, ppl=1.2978\n",
      "iter 1: dev loss/word=0.2641, ppl=1.3022, time=1.44s\n",
      "edward jointly in boston corp. poland that 's kellogg in a mechanism for his biggest steps successful for its third year concentrate\n",
      "it 's harris international operations of using technological in N million shares without the <unk> two-year <unk> about N N in beyond and economic reforms in stocks\n",
      "sen. resistance t. other corp. processing companies on a units discussed their one-third has to pay\n",
      "it could n't be reached minnesota in the opposition era\n",
      "there are n't same finances is backed in frankfurt to encourage an fairly computer denied sansui of design forces we had once nine months america society of the sense the estimated volume was # N million\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 2: train loss/word=0.2536, ppl=1.2886\n",
      "iter 2: dev loss/word=0.2624, ppl=1.3000, time=1.43s\n",
      "him as european and some movie come on more <unk>\n",
      "jaguar officials had N returned\n",
      "elsewhere developer 's season conditions the him of damage to pressing newly <unk> independent available <unk> the circuit court because it meet the remaining gold volume N shares\n",
      "i gave it changed offered richard stake in trouble\n",
      "next never has been seems to make out of for the venture\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 3: train loss/word=0.2495, ppl=1.2834\n",
      "iter 3: dev loss/word=0.2617, ppl=1.2992, time=1.42s\n",
      "in the network for a second year because of problems citing with three other double-digit institutions by not 's a <unk> is that it does n't do any job about global inc\n",
      "in morris <unk> of next year\n",
      "a seat on the field location to south carolina and berlin say they will remain also such <unk> inside there were a bit of the third quarter amounted to technical feeling a <unk> senate have for the british air congress he said\n",
      "but N off N\n",
      "than\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 4: train loss/word=0.2465, ppl=1.2795\n",
      "iter 4: dev loss/word=0.2611, ppl=1.2984, time=1.46s\n",
      "<unk>\n",
      "here is offering\n",
      "there is no effect began program no one longer trade after the government coverage\n",
      "mr. <unk> recalls\n",
      "but not money-market for $ N billion in cd\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 5: train loss/word=0.2442, ppl=1.2766\n",
      "iter 5: dev loss/word=0.2606, ppl=1.2978, time=1.41s\n",
      "japanese brokerage firms might be <unk>\n",
      "we 're <unk> to ground by <unk> ag <unk> at cheap near players\n",
      "rally democrats are the nearly N large canada of <unk>\n",
      "the junk market who put and common <unk> in give the u.s. have been <unk> in the principal against what the just veteran party net nobody will all your father was some interest-rate fund community\n",
      "of the law also will be named many president of the $ N billion and levels\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 6: train loss/word=0.2423, ppl=1.2742\n",
      "iter 6: dev loss/word=0.2603, ppl=1.2973, time=1.42s\n",
      "house agreed to <unk> anything was taken next week\n",
      "ambassador to sound old <unk> registered inc. in <unk> additional issues options at new york stock exchange rose to $ N billion this week and lawmakers are not expected\n",
      "the two-thirds of hard this goes big source\n",
      "inco 's proposal to find appointed even\n",
      "when the <unk> district\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 7: train loss/word=0.2407, ppl=1.2721\n",
      "iter 7: dev loss/word=0.2596, ppl=1.2964, time=1.42s\n",
      "many banks believe mr. <unk> lives in september bought last month saying to launch by program trading in the letters with the out and sell or the yields on 30-year mortgage commitments for delivery within N days N N N N to $ N million\n",
      "daily never william c. hunt bell to become a <unk> <unk> las vegas said\n",
      "but its corporate giant increasing a u.s. can farm majority employee the department to stay solely unit stake\n",
      "the <unk> <unk>\n",
      "justice as one\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 8: train loss/word=0.2393, ppl=1.2704\n",
      "iter 8: dev loss/word=0.2601, ppl=1.2970, time=1.44s\n",
      "among simply control rose to other offering an irs\n",
      "but he said\n",
      "the federal drug recognized and his private department store\n",
      "he would be waste\n",
      "mr. <unk> said the move has a computer maker familiar with the nikkei index can go out\n",
      "--finished 5000 sentences\n",
      "--finished 10000 sentences\n",
      "--finished 15000 sentences\n",
      "--finished 20000 sentences\n",
      "--finished 25000 sentences\n",
      "--finished 30000 sentences\n",
      "--finished 35000 sentences\n",
      "--finished 40000 sentences\n",
      "iter 9: train loss/word=0.2381, ppl=1.2688\n",
      "iter 9: dev loss/word=0.2596, ppl=1.2965, time=1.43s\n",
      "there and <unk> a cases even older to be based on the leading <unk> and national patterns on the big also now stands what 's big institutional through <unk> the death of <unk> units and painewebber inc. a ads in N for act as the possible offer via painewebber group inc. said it is n't just <unk> would make it must be fully operational for a drug <unk>\n",
      "estimated market fundamental administration common stocks\n",
      "gerald never owned by the end of a previous five years of if he would business and its shareholders remainder in the fourth idea was up from N N to N N\n",
      "the corporate certificates of capital spending from the residents and N units with the executives american appear <unk>\n",
      "mr. mitchell 's national guber son to the institutional investors ' appeared to pass it was the prices must <unk> him as <unk>\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for ITER in range (10): # CHANGE to 100\n",
    "    # training\n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    for sent_id, sent in enumerate(train_data):\n",
    "        \n",
    "        my_loss = calc_sent_loss(sent)\n",
    "\n",
    "        train_loss += my_loss.item()\n",
    "        train_words += len(sent)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished %r sentences\" % (sent_id+1))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words)))\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(val_data):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.item()\n",
    "        dev_words += len(sent)\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
    "\n",
    "    # Generate a few sentences\n",
    "    for _ in range(5):\n",
    "        sent = generate_sent()\n",
    "        print(\" \".join([index_to_word[x] for x in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "154abf72fb8cc0db1aa0e7366557ff891bff86d6d75b7e5f2e68a066d591bfd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
